The final parameters of the XGBoost model are:

    XGBClassifier(
	base_score=0.5, 
	booster='gbtree', 
	colsample_bylevel=1,
	colsample_bynode=1, 
	colsample_bytree=1, 
	gamma=0, 
	gpu_id=-1,
    importance_type='gain', 
	interaction_constraints='',
    learning_rate=0.2, 
	max_delta_step=0, 
	max_depth=6,
    min_child_weight=1, 
	missing=nan, 
	monotone_constraints='()',
	n_estimators=140, 
	n_jobs=6, 
	num_parallel_tree=1,
    objective='multi:softprob', 
	random_state=42, 
	reg_alpha=0,
    reg_lambda=1, 
	scale_pos_weight=None, 
	seed=42, subsample=1,
    tree_method='exact', 
	use_label_encoder=False,
	validate_parameters=1, 
	verbosity=None)
	

Best Parameters -  {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 140}

              precision    recall  f1-score   support

           0       0.96      0.98      0.97       494
           1       0.87      0.78      0.83        88
           2       0.98      0.98      0.98        52

    accuracy                           0.95       634
   macro avg       0.94      0.92      0.93       634
weighted avg       0.95      0.95      0.95       634


		
All features standard scaled

	
